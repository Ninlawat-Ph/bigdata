## วิธีใช้โมเดลการเรียนรู้ของเครื่องเพื่อทำนายการสตรีมข้อมูลโดยใช้ PySpark
โดยอ้างอิงมาจาก https://www.analyticsvidhya.com/blog/2019/12/streaming-data-pyspark-machine-learning-model/

#### Overview 
-	Streaming data เป็นหลักการที่น่าตื่นเต้นของ machine learning
-	เรียนรู้วิธีการใช้ machine learning (logistic regression) เพื่อทำนาย Streaming data โดยใช้ PySpark
-	ครอบคลุมพื้นฐานของ Streaming data และ Spark Streaming

#### Introduction
- ทุกวินาทีมีการส่ง Tweets มากกว่า 8,500 Tweets รูปภาพ มากกว่า 900 รูปภาพถูกอัพโหลดบน Instagram มีการโทร Skype มากกว่า 4,200 ครั้ง มีการโทรค้นหาโดย google มากกว่า 78,000 ครั้งและมีการส่ง email มากกว่า 2 ล้านฉบับ
- เราจะรวบรวมข้อมูลในระดับนี้ได้อย่างไร เราจะรู้ได้อย่างไรว่า machine learning pipeline  ยังคงประมวลผลลลัพธ์ทันที่มีการสร้างและรวบรวมข้อมูล สิ่งเหล่านี้เป็นงานสำคัญในอุตสาหกกรมที่ท้าทายที่ต้องเผชิญ และ เหตุใดแนวคิด Streaming data จึงได้รับความสนใจมากขึ้นระหว่างองค์กร
- ดังนั้นในบทความนี้เราจะได้เรียนรู้ว่าข้อมูลแบบสตรีมคืออะไรเข้าใจพื้นฐานของการสตรีมแบบ Spark แล้วทำงานกับชุดข้อมูลที่เกี่ยวข้องกับอุตสาหกรรมเพื่อนำข้อมูลแบบสตรีมมิ่งไปใช้โดยใช้ Spark

#### What is Streaming Data?
- Streaming data ไม่มีจุดเริ่มต้นหรือจุดสิ้นสุดแบบต่อเนื่อง ข้อมูลนี้สร้างขึ้นทุกๆ วินาที จากข้อมูลหลายแหล่งและจำเป็นต้องประมวลผลและวิเคราะห์โดยเร็วที่สุด ข้อมูลสตรีมจำนวนมากต้องได้รับการประมวลผลแบบ real-time เช่นการค้นหาของ Google 
#### Fundamentals of Spark Streaming
- Spark Streaming เป็นส่วนเสริมของ Core Spark API ที่ช่วยให้สามารถปนะมวลผลสตรีมข้อมูลล่าสุดที่ปรับขนาดได้และไม่ผิดพลาดมาทำความเข้าใจกับองค์ประกอบต่าง ๆ ของ Spark Streaming
1. Discretized Streams
 Discretized Streams, or DStreams, represent a continuous stream of data 
 
      ขั้นตอนแรกของการสร้างแอพพลิเคชั่นสตรีมมิ่งคือการกำหนดระยะเวลาแบทช์สำหรับ  แหล่งข้อมูลที่เรารวบรวมข้อมูล หากระยะเวลาแบทช์คือ 2 วินาทีข้อมูลจะถูกเก็บรวบรวมทุก     2 วินาทีและเก็บไว้ใน RDD และสายโซ่ของซีรีย์ต่อเนื่องของ RDD เหล่านี้คือ DStream      ซึ่งไม่เปลี่ยนรูปและสามารถใช้เป็นชุดข้อมูลแบบกระจายโดย Spark ##
 
    ในระหว่างขั้นตอนการประมวลผลข้อมูลล่วงหน้าเรา จำเป็นต้อง  transform variables, including converting categorical ones into numeric, creating        bins ลบค่าผิดปกติ Spark เก็บข้อมูลประวัติการ transformations ที่เรากำหนดไว้ในข้อมูลใดๆ ดังนั้นเมื่อใดก็ตามที่มีความผิดพลาดเกิดขึ้นมันสามารถย้อนเส้นทางของ         transformations และการสร้างผลลัพธ์ที่คำนวณใหม่อีกครั้ง

    เราต้องการให้แอปพลิเคชั่น Spark ของเราทำงาน 24 x 7 และเมื่อใดก็ตามที่มีข้อผิดพลาดเกิดขึ้นเราต้องการให้มันกู้คืนโดยเร็วที่สุด แต่ในขณะที่ทำงานกับข้อมูลขนาดใหญ่        Spark จำเป็นต้องคำนวณการแปลงทั้งหมดอีกครั้งในกรณีที่มีข้อผิดพลาด อย่างที่คุณจินตนาการได้อาจมีราคาค่อนข้างสูง

2. Caching
   นี่คือวิธีหนึ่งในการจัดการกับความท้าทายนี้ เราสามารถเก็บผลลัพธ์ที่เราคำนวณ (แคช) ชั่วคราวเพื่อรักษาผลลัพธ์ของการแปลงที่กำหนดไว้ในข้อมูล 
   ด้วยวิธีนี้เราไม่ต้องคำนวณการแปลงเหล่านั้นซ้ำแล้วซ้ำอีกเมื่อเกิดความผิดพลาดใด ๆ DStreams ช่วยให้เราสามารถเก็บข้อมูลสตรีมในหน่วยความจำ สิ่งนี้มีประโยชน์เมื่อเราต้องการคำนวณการดำเนินการหลายอย่างในข้อมูลเดียวกัน

3. Checkpointing
   การแคชมีประโยชน์อย่างยิ่งเมื่อเราใช้อย่างถูกต้อง แต่ต้องการหน่วยความจำจำนวนมาก และไม่ใช่ทุกคนที่มีเครื่องหลายร้อยเครื่องที่มี RAM ขนาด 128 GB เพื่อแคชทุกอย่าง
   
   Checkpointing คืออีกเทคนิคหนึ่งในการเก็บผลลัพธ์ของข้อมูลเฟรมที่ถูก transformation แล้วมันช่วยประหยัดสถานะของแอพพลิเคชั่นที่รันเป็นระยะ ๆ บนที่เก็บ reliable    ข้อมูลคล้าย HDFS อย่างไรก็ตามมันช้าลงและมีความยืดหยุ่นน้อยกว่าแคช
   
   เราสามารถใช้ Checkpointing เมื่อเรามีการสตรีมข้อมูล ผลลัพธ์การเปลี่ยนแปลงขึ้นอยู่กับผลลัพธ์การแปลงก่อนหน้าและจำเป็นต้องเก็บรักษาไว้เพื่อใช้งาน นอกจากนี้เรายังตรวจสอบข้อมูลเมตาดาต้าเช่นเดียวกับการกำหนดค่าที่ใช้ในการสร้างข้อมูลสตรีมมิ่งและผลลัพธ์ของชุดการดำเนินการ DStream

4. Shared Variables in Streaming Data
   มีบางครั้งที่เราจำเป็นต้องกำหนดฟังก์ชั่นเช่นแผนที่ลดหรือกรองสำหรับแอปพลิเคชัน Spark ของเราที่จะต้องดำเนินการในหลาย ๆ กลุ่ม ตัวแปรที่ใช้ในฟังก์ชั่นนี้จะถูกคัดลอกไปยัง     แต่ละเครื่อง (กลุ่ม)
   
    ที่นี่แต่ละคลัสเตอร์มีตัวดำเนินการแตกต่างกันและเราต้องการบางสิ่งที่สามารถให้ความสัมพันธ์ระหว่างตัวแปรเหล่านี้กับเราตัวอย่างเช่นสมมติว่าแอปพลิเคชัน Spark ของเราทำงานใน   100 กลุ่มที่แตกต่างกันซึ่งจับภาพ Instagram ที่โพสต์โดยผู้คนจากประเทศต่างๆ เราต้องนับแท็กเฉพาะที่กล่าวถึงใน โพสต์

    ตอนนี้ executors ของแต่ละคลัสเตอร์จะคำนวณผลลัพธ์ของข้อมูลที่มีอยู่ในคลัสเตอร์นั้น ๆ แต่เราต้องการบางสิ่งที่ช่วยให้กลุ่มเหล่านี้สื่อสารกันเพื่อที่เราจะได้รับผลสรุปรวม ใน Spark เรามีตัวแปรที่แชร์ซึ่งทำให้เราสามารถแก้ไขปัญหานี้ได้

5. Accumulator Variable
  ตัวจัดการบนแต่ละคลัสเตอร์จะส่งข้อมูลกลับไปยังกระบวนการของไดรเวอร์เพื่ออัพเดทค่าของตัวแปรตัวสะสม โดยจะมีตัวแปรที่ใช้สำหรับรวบรวมข้อมูล executors เรียกตัวแปรนี้ว่า Accumulator
  
6. Broadcast Variable
   Broadcast Variable อนุญาตให้โปรแกรมเมอร์เก็บตัวแปรแคชแบบอ่านอย่างเดียวในแต่ละเครื่อง โดยปกติ Spark จะกระจาย Broadcast Variable โดยอัตโนมัติโดยใช้อัลกอริธึมการ Broadcast ที่มีประสิทธิภาพ แต่เรายังสามารถกำหนดได้หากเรามีงานที่ต้องใช้ข้อมูลเดียวกันหลายขั้นตอน
  
#### Understanding the Problem Statement
 - งานคือการจัดหมวดหมู่ Tweets แบ่งแยกเชื้อชาติหรือกีดกันทางเพศจากทวีตอื่น ๆ เราจะใช้ข้อมูลตัวอย่างการฝึกอบรมของ Tweets Label '1' แสดงว่าทวีตนั้นเป็นชนชั้น    เหยียด   ผิว / เพศหญิงและ Lable '0'

#### Setting up the Project Workflow
1. Model Building:  เราจะสร้าง Logistic Regression Model pipeline เพื่อจำแนกว่าทวีตมีคำพูดแสดงความเกลียดชังหรือไม่ ที่นี่เรามุ่งเน้นที่จะไม่สร้างรูปแบบการจัดประเภทที่แม่นยำมาก แต่เพื่อดูวิธีการใช้แบบจำลองใด ๆ และส่งคืนผลข้อมูลการสตรีม

2. Initialize Spark Streaming Context: เมื่อสร้างแบบจำลองแล้วเราจำเป็นต้องกำหนดชื่อโฮสต์และหมายเลขพอร์ตจากตำแหน่งที่เรารับข้อมูลสตรีม

3. Stream Data: ต่อไปเราจะเพิ่มทวีตจากเซิร์ฟเวอร์ netcat จากพอร์ตที่กำหนดและ Spark Streaming API จะได้รับข้อมูลหลังจากระยะเวลาที่กำหนด

4. Predict and Return Results: เมื่อเราได้รับข้อความ Tweets เราจะส่งข้อมูลไปยังขั้นตอนการเรียนรู้ของเครื่องที่เราสร้างขึ้นและส่งคืนความรู้สึกที่คาดการณ์จากแบบจำลอง

![Push up to github](https://cdn.analyticsvidhya.com/wp-content/uploads/2019/12/overview.png)

#### Defining the Stages of our Machine Learning Pipeline
- ตอนนี้เรามีข้อมูลใน Spark dataframe แล้วเราต้องกำหนด stage ต่าง ๆ ที่เราต้องการแปลงข้อมูลจากนั้นใช้มันเพื่อรับ label ที่ถูกทำนายจากโมเดลของเรา
ใน stage แรกเราจะใช้ RegexTokenizer เพื่อแปลงข้อความ Tweets เป็นรายการของคำ จากนั้นเราจะลบคำหยุดออกจาก word list และ word vectors ในขั้นตอนสุดท้ายเราจะใช้คำว่าเวกเตอร์เหล่านี้เพื่อสร้างแบบจำลองการถดถอยโลจิสติกส์(logistic regression model)และรับความรู้สึกที่ทำนายไว้

![Push up to github](https://cdn.analyticsvidhya.com/wp-content/uploads/2019/12/pipeline_streaming.png)

#### Setup our Machine Learning Pipeline
- เพิ่มขั้นตอนในวัตถุ Pipeline  แล้วเราจะทำการแปลงตามลำดับ ติดตั้ง Pipeline  กับชุดข้อมูลการฝึกอบรมและตอนนี้เมื่อใดก็ตามที่เรามี Tweets ใหม่เราเพียงแค่ต้องส่งผ่านวัตถุ Tweets และแปลงข้อมูลเพื่อรับการคาดการณ์
     
       #setup the pipeline
       pipeline = Pipeline(stages= [stage_1, stage_2, stage_3, model])

       #fit the pipeline model with the training data
       pipelineFit = pipeline.fit(my_data)

